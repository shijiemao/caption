{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ea5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DataCollatorForSeq2Seq, AdamWeightDecay, \\\n",
    "    T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f339f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\" Use tokenizer to preprocess data. \"\"\"\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    prefix = \"summarize: \"\n",
    "\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlights\"], max_length=80, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def download_and_preprocess_data():\n",
    "    \"\"\" Load dataset from HuggingFace and preprocess. \"\"\"\n",
    "    \n",
    "    news_ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "\n",
    "    # Tokenized using preprocess_function\n",
    "    tokenized_news = news_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "    return tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4c3146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a48bc8dd0d84c10a79190b9e7445921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",from_pt = True)\n",
    "\n",
    "optimizer = AdamWeightDecay(\n",
    "    learning_rate=2e-5, \n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aed1c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/28165/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "Loading cached processed dataset at C:\\Users\\28165\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\\cache-eba88d0ba3636bf1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 11490\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_news = download_and_preprocess_data()\n",
    "tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "712a17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tokenized_news.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b2c41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(metric, pred, actual):\n",
    "    \"\"\" Compute the model's rouge performance on an instance. \"\"\"\n",
    "\n",
    "    metric.add(predictions=pred, references=actual)\n",
    "    final_score = metric.compute()\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "019d219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 100\n",
      "Round: 200\n",
      "Round: 300\n",
      "Round: 400\n",
      "Round: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dd80fbf68f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     pred = model.generate(\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdo_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m         )\n\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m         return self._generate_beam_search(\n\u001b[0m\u001b[0;32m    944\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m             \u001b[0mcur_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[1;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, encoder_outputs, attention_mask, use_cache, forced_bos_token_id, forced_eos_token_id, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbanned_tokens_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbanned_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     banned_tokens_indices_mask.append(\n\u001b[1;32m-> 1113\u001b[1;33m                         \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbanned_tokens_slice\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m                     )\n\u001b[0;32m   1115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbanned_tokens_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbanned_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     banned_tokens_indices_mask.append(\n\u001b[1;32m-> 1113\u001b[1;33m                         \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbanned_tokens_slice\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m                     )\n\u001b[0;32m   1115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metric = load_metric('rouge')\n",
    "result = [[] for x in range(3)]\n",
    "\n",
    "cnt = 0\n",
    "for item in test_ds:\n",
    "    article = item['input_ids']\n",
    "    actual = item['labels']\n",
    "    \n",
    "    pred = model.generate(\n",
    "        do_sample=True,\n",
    "        input_ids=article,\n",
    "        # min_length=56,\n",
    "        max_length=80,\n",
    "        temperature=0.8, \n",
    "        top_k=45,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    rouge_score = compute_metrics(metric, pred, actual)\n",
    "    rouge1 = 100 * rouge_score['rouge1'][1][2]\n",
    "    rouge2 = 100 * rouge_score['rouge2'][1][2]\n",
    "    rougeL = 100 * rouge_score['rougeL'][1][2]\n",
    "\n",
    "    cnt += 1 \n",
    "    if cnt % 25 == 0:\n",
    "        print(f'Round: {cnt * 4}')\n",
    "\n",
    "    result[0].append(rouge1)\n",
    "    result[1].append(rouge2)\n",
    "    result[2].append(rougeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a96ec150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.30434782608695,\n",
       " 45.86206896551724,\n",
       " 41.9047619047619,\n",
       " 30.0632911392405,\n",
       " 34.66666666666667,\n",
       " 37.919463087248324,\n",
       " 33.45323741007194,\n",
       " 38.666666666666664,\n",
       " 37.85714285714286,\n",
       " 35.815602836879435,\n",
       " 32.857142857142854,\n",
       " 31.967213114754095,\n",
       " 36.394557823129254,\n",
       " 35.56338028169014,\n",
       " 40.789473684210535,\n",
       " 36.61971830985915,\n",
       " 40.26845637583892,\n",
       " 40.833333333333336,\n",
       " 33.55263157894737,\n",
       " 38.16793893129771,\n",
       " 40.0,\n",
       " 35.416666666666664,\n",
       " 31.25,\n",
       " 29.411764705882355,\n",
       " 33.33333333333333,\n",
       " 37.5,\n",
       " 34.96503496503497,\n",
       " 34.10852713178294,\n",
       " 32.16783216783217,\n",
       " 42.857142857142854,\n",
       " 33.56164383561644,\n",
       " 30.47945205479452,\n",
       " 35.15625000000001,\n",
       " 35.338345864661655,\n",
       " 39.310344827586206,\n",
       " 37.03703703703703,\n",
       " 28.47682119205298,\n",
       " 36.59420289855072,\n",
       " 38.43283582089552,\n",
       " 30.41666666666667,\n",
       " 36.56716417910447,\n",
       " 36.0,\n",
       " 30.14705882352941,\n",
       " 44.26229508196722,\n",
       " 33.587786259541986,\n",
       " 32.22222222222222,\n",
       " 33.44594594594595,\n",
       " 33.54430379746836,\n",
       " 34.0625,\n",
       " 30.718954248366014,\n",
       " 35.08064516129032,\n",
       " 29.78723404255319,\n",
       " 34.751773049645394,\n",
       " 38.65248226950355,\n",
       " 34.93589743589743,\n",
       " 35.12658227848101,\n",
       " 37.49999999999999,\n",
       " 41.91176470588235,\n",
       " 36.95652173913044,\n",
       " 39.77272727272727,\n",
       " 47.810218978102185,\n",
       " 34.070796460176986,\n",
       " 34.172661870503596,\n",
       " 36.84210526315789,\n",
       " 31.751824817518248,\n",
       " 43.83116883116883,\n",
       " 39.23611111111111,\n",
       " 38.888888888888886,\n",
       " 38.93129770992367,\n",
       " 37.77777777777778,\n",
       " 35.984848484848484,\n",
       " 32.35294117647059,\n",
       " 35.625,\n",
       " 33.739837398373986,\n",
       " 35.08064516129033,\n",
       " 26.89393939393939,\n",
       " 41.08527131782946,\n",
       " 40.55944055944056,\n",
       " 36.267605633802816,\n",
       " 38.983050847457626,\n",
       " 44.3089430894309,\n",
       " 32.41379310344827,\n",
       " 42.405063291139236,\n",
       " 36.16352201257862,\n",
       " 32.945736434108525,\n",
       " 37.857142857142854,\n",
       " 40.234375,\n",
       " 26.923076923076927,\n",
       " 31.944444444444443,\n",
       " 36.91588785046729,\n",
       " 41.791044776119406,\n",
       " 36.940298507462686,\n",
       " 40.06849315068492,\n",
       " 34.14634146341463,\n",
       " 36.971830985915496,\n",
       " 37.8125,\n",
       " 36.51315789473684,\n",
       " 39.28571428571429,\n",
       " 33.21917808219178,\n",
       " 34.516129032258064,\n",
       " 39.93506493506494,\n",
       " 27.814569536423843,\n",
       " 25.524475524475527,\n",
       " 35.51724137931034,\n",
       " 24.060150375939852,\n",
       " 46.21212121212121,\n",
       " 40.2027027027027,\n",
       " 38.33333333333333,\n",
       " 33.33333333333333,\n",
       " 42.53246753246753,\n",
       " 40.0,\n",
       " 36.82432432432432,\n",
       " 38.93129770992366,\n",
       " 32.01754385964912,\n",
       " 36.496350364963504,\n",
       " 34.92063492063492,\n",
       " 28.525641025641026,\n",
       " 31.329113924050638,\n",
       " 31.6,\n",
       " 35.24590163934426,\n",
       " 36.15384615384616,\n",
       " 32.692307692307686,\n",
       " 32.846715328467155,\n",
       " 45.77922077922078,\n",
       " 31.59722222222222,\n",
       " 36.09022556390977,\n",
       " 43.359375,\n",
       " 37.596899224806194,\n",
       " 34.121621621621614,\n",
       " 36.25,\n",
       " 44.907407407407405,\n",
       " 39.90384615384615,\n",
       " 28.014184397163117,\n",
       " 31.007751937984494,\n",
       " 38.333333333333336,\n",
       " 36.46616541353383,\n",
       " 34.66666666666667,\n",
       " 26.22950819672131,\n",
       " 30.844155844155846,\n",
       " 33.47457627118644,\n",
       " 37.943262411347526]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cd1e508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
