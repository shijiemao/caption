{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7901317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#Common Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e33184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792k/792k [00:00<00:00, 10.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Data Transform\n",
    "import torchtext\n",
    "from torchtext.prototype.models import T5Transform\n",
    "\n",
    "padding_idx = 0\n",
    "eos_idx = 1\n",
    "max_seq_len = 512\n",
    "t5_sp_model_path = \"https://download.pytorch.org/models/text/t5_tokenizer_base.model\"\n",
    "\n",
    "transform = T5Transform(\n",
    "    sp_model_path=t5_sp_model_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    eos_idx=eos_idx,\n",
    "    padding_idx=padding_idx,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/text/t5.base.generation.pt\" to C:\\Users\\28165/.cache\\torch\\hub\\checkpoints\\t5.base.generation.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cd7e086c84a73b197b38d04d32fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/945M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (token_embeddings): Embedding(32128, 768, padding_idx=0)\n",
       "  (encoder): T5Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "          (relative_attention_bias): Embedding(32, 12)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (8): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (9): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (10): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (11): T5EncoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm1): T5LayerNorm()\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (decoder): T5Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "          (relative_attention_bias): Embedding(32, 12)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (8): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (9): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (10): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (11): T5DecoderLayer(\n",
       "        (self_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        (norm1): T5LayerNorm()\n",
       "        (norm2): T5LayerNorm()\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (cross_attn): T5MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm3): T5LayerNorm()\n",
       "        (dropout4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm2): T5LayerNorm()\n",
       "  (dropout3): Dropout(p=0.0, inplace=False)\n",
       "  (dropout4): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Preparation\n",
    "\n",
    "from torchtext.prototype.models import T5_BASE_GENERATION\n",
    "\n",
    "\n",
    "t5_base = T5_BASE_GENERATION\n",
    "transform = t5_base.transform()\n",
    "model = t5_base.get_model()\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4155e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torchtext.prototype.models import T5Model\n",
    "\n",
    "\n",
    "def beam_search(\n",
    "    beam_size: int,\n",
    "    step: int,\n",
    "    bsz: int,\n",
    "    decoder_output: Tensor,\n",
    "    decoder_tokens: Tensor,\n",
    "    scores: Tensor,\n",
    "    incomplete_sentences: Tensor,\n",
    "):\n",
    "    probs = F.log_softmax(decoder_output[:, -1], dim=-1)\n",
    "    top = torch.topk(probs, beam_size)\n",
    "\n",
    "    # N is number of sequences in decoder_tokens, L is length of sequences, B is beam_size\n",
    "    # decoder_tokens has shape (N,L) -> (N,B,L)\n",
    "    # top.indices has shape (N,B) - > (N,B,1)\n",
    "    # x has shape (N,B,L+1)\n",
    "    # note that when step == 1, N = batch_size, and when step > 1, N = batch_size * beam_size\n",
    "    x = torch.cat([decoder_tokens.unsqueeze(1).repeat(1, beam_size, 1), top.indices.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # beams are first created for a given sequence\n",
    "    if step == 1:\n",
    "        # x has shape (batch_size, B, L+1) -> (batch_size * B, L+1)\n",
    "        # new_scores has shape (batch_size,B)\n",
    "        # incomplete_sentences has shape (batch_size * B) = (N)\n",
    "        new_decoder_tokens = x.view(-1, step + 1)\n",
    "        new_scores = top.values\n",
    "        new_incomplete_sentences = incomplete_sentences\n",
    "\n",
    "    # beams already exist, want to expand each beam into possible new tokens to add\n",
    "    # and for all expanded beams beloning to the same sequences, choose the top k\n",
    "    else:\n",
    "        # scores has shape (batch_size,B) -> (N,1) -> (N,B)\n",
    "        # top.values has shape (N,B)\n",
    "        # new_scores has shape (N,B) -> (batch_size, B^2)\n",
    "        new_scores = (scores.view(-1, 1).repeat(1, beam_size) + top.values).view(bsz, -1)\n",
    "\n",
    "        # v, i have shapes (batch_size, B)\n",
    "        v, i = torch.topk(new_scores, beam_size)\n",
    "\n",
    "        # x has shape (N,B,L+1) -> (batch_size, B, L+1)\n",
    "        # i has shape (batch_size, B) -> (batch_size, B, L+1)\n",
    "        # new_decoder_tokens has shape (batch_size, B, L+1) -> (N, L)\n",
    "        x = x.view(bsz, -1, step + 1)\n",
    "        new_decoder_tokens = x.gather(index=i.unsqueeze(-1).repeat(1, 1, step + 1), dim=1).view(-1, step + 1)\n",
    "\n",
    "        # need to update incomplete sentences in case one of the beams was kicked out\n",
    "        # y has shape (N) -> (N, 1) -> (N, B) -> (batch_size, B^2)\n",
    "        y = incomplete_sentences.unsqueeze(-1).repeat(1, beam_size).view(bsz, -1)\n",
    "\n",
    "        # now can use i to extract those beams that were selected\n",
    "        # new_incomplete_sentences has shape (batch_size, B^2) -> (batch_size, B) -> (N, 1) -> N\n",
    "        new_incomplete_sentences = y.gather(index=i, dim=1).view(bsz * beam_size, 1).squeeze(-1)\n",
    "\n",
    "        # new_scores has shape (batch_size, B)\n",
    "        new_scores = v\n",
    "\n",
    "    return new_decoder_tokens, new_scores, new_incomplete_sentences\n",
    "\n",
    "\n",
    "def generate(encoder_tokens: Tensor, eos_idx: int, model: T5Model, beam_size: int) -> Tensor:\n",
    "\n",
    "    # pass tokens through encoder\n",
    "    bsz = encoder_tokens.size(0)\n",
    "    encoder_padding_mask = encoder_tokens.eq(model.padding_idx)\n",
    "    encoder_embeddings = model.dropout1(model.token_embeddings(encoder_tokens))\n",
    "    encoder_output = model.encoder(encoder_embeddings, tgt_key_padding_mask=encoder_padding_mask)[0]\n",
    "\n",
    "    encoder_output = model.norm1(encoder_output)\n",
    "    encoder_output = model.dropout2(encoder_output)\n",
    "\n",
    "    # initialize decoder input sequence; T5 uses padding index as starter index to decoder sequence\n",
    "    decoder_tokens = torch.ones((bsz, 1), dtype=torch.long) * model.padding_idx\n",
    "    scores = torch.zeros((bsz, beam_size))\n",
    "\n",
    "    # mask to keep track of sequences for which the decoder has not produced an end-of-sequence token yet\n",
    "    incomplete_sentences = torch.ones(bsz * beam_size, dtype=torch.long)\n",
    "\n",
    "    # iteratively generate output sequence until all sequences in the batch have generated the end-of-sequence token\n",
    "    for step in range(model.config.max_seq_len):\n",
    "\n",
    "        if step == 1:\n",
    "            # duplicate and order encoder output so that each beam is treated as its own independent sequence\n",
    "            new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "            new_order = new_order.to(encoder_tokens.device).long()\n",
    "            encoder_output = encoder_output.index_select(0, new_order)\n",
    "            encoder_padding_mask = encoder_padding_mask.index_select(0, new_order)\n",
    "\n",
    "        # causal mask and padding mask for decoder sequence\n",
    "        tgt_len = decoder_tokens.shape[1]\n",
    "        decoder_mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.float64), diagonal=1).bool()\n",
    "        decoder_padding_mask = decoder_tokens.eq(model.padding_idx)\n",
    "\n",
    "        # T5 implemention uses padding idx to start sequence. Want to ignore this when masking\n",
    "        decoder_padding_mask[:, 0] = False\n",
    "\n",
    "        # pass decoder sequence through decoder\n",
    "        decoder_embeddings = model.dropout3(model.token_embeddings(decoder_tokens))\n",
    "        decoder_output = model.decoder(\n",
    "            decoder_embeddings,\n",
    "            memory=encoder_output,\n",
    "            tgt_mask=decoder_mask,\n",
    "            tgt_key_padding_mask=decoder_padding_mask,\n",
    "            memory_key_padding_mask=encoder_padding_mask,\n",
    "        )[0]\n",
    "\n",
    "        decoder_output = model.norm2(decoder_output)\n",
    "        decoder_output = model.dropout4(decoder_output)\n",
    "        decoder_output = decoder_output * (model.config.embedding_dim ** -0.5)\n",
    "        decoder_output = model.lm_head(decoder_output)\n",
    "\n",
    "        decoder_tokens, scores, incomplete_sentences = beam_search(\n",
    "            beam_size, step + 1, bsz, decoder_output, decoder_tokens, scores, incomplete_sentences\n",
    "        )\n",
    "        # ignore newest tokens for sentences that are already complete\n",
    "        decoder_tokens[:, -1] *= incomplete_sentences\n",
    "\n",
    "        # update incomplete_sentences to remove those that were just ended\n",
    "        incomplete_sentences = incomplete_sentences - (decoder_tokens[:, -1] == eos_idx).long()\n",
    "\n",
    "        # early stop if all sentences have been ended\n",
    "        if (incomplete_sentences == 0).all():\n",
    "            break\n",
    "\n",
    "    # take most likely sequence\n",
    "    decoder_tokens = decoder_tokens.view(bsz, beam_size, -1)[:, 0, :]\n",
    "    return decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f4e867",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IterableWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7917de5b3eb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcnndm_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcnndm_datapipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNNDM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"summarize\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchtext\\data\\datasets_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(root, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchtext\\data\\datasets_utils.py\u001b[0m in \u001b[0;36mnew_fn\u001b[1;34m(root, split, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_check_default_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrap_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchtext\\datasets\\cnndm.py\u001b[0m in \u001b[0;36mCNNDM\u001b[1;34m(root, split)\u001b[0m\n\u001b[0;32m    145\u001b[0m         )\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[0mcnn_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cnn\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[0mdailymail_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dailymail\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mdata_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_dp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdailymail_dp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchtext\\datasets\\cnndm.py\u001b[0m in \u001b[0;36m_load_stories\u001b[1;34m(root, source, split)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0msplit_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_split_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mstory_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIterableWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     cache_compressed_dp = story_dp.on_disk_cache(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchtext\\datasets\\cnndm.py\u001b[0m in \u001b[0;36m_get_split_list\u001b[1;34m(source, split)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_split_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0murl_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIterableWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSPLIT_LIST\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0monline_dp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOnlineReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_dp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0monline_dp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_hash_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IterableWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "\n",
    "from functools import partial\n",
    "from torchtext.datasets import CNNDM\n",
    "\n",
    "cnndm_batch_size = 5\n",
    "cnndm_datapipe = CNNDM(split=\"test\")\n",
    "task = \"summarize\"\n",
    "\n",
    "\n",
    "def apply_prefix(task, x):\n",
    "    return f\"{task}: \" + x[0], x[1]\n",
    "\n",
    "\n",
    "cnndm_datapipe = cnndm_datapipe.map(partial(apply_prefix, task))\n",
    "cnndm_datapipe = cnndm_datapipe.batch(cnndm_batch_size)\n",
    "cnndm_datapipe = cnndm_datapipe.rows2columnar([\"article\", \"abstract\"])\n",
    "cnndm_dataloader = DataLoader(cnndm_datapipe, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac4d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.1\n"
     ]
    }
   ],
   "source": [
    "import torchdata\n",
    "print(torchdata.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be4d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
