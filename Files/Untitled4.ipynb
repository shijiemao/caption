{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7c104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"result_2.csv\")\n",
    "\n",
    "new = df[\"new_string\"]\n",
    "target = df['String']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d38d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "all = []\n",
    "for i in range(len(new)):\n",
    "    \n",
    "    scores = scorer.score(new[i],target[i])\n",
    "    all.append(scores['rouge1'][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cfe2c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4615384615384615, 0.09090909090909091, 0.14285714285714288, 0.11764705882352942, 0.33333333333333326, 0.0, 0.3529411764705882, 0.2727272727272727, 0.18181818181818182, 0.30769230769230765, 0.30769230769230765, 0.3, 0.33333333333333326, 0.14285714285714285, 0.35714285714285715, 0.3, 0.42857142857142855, 0.0, 0.30769230769230765, 0.10526315789473685, 0.15384615384615385, 0.11764705882352941, 0.23529411764705885, 0.19999999999999998, 0.08695652173913045, 0.33333333333333326, 0.0, 0.25000000000000006, 0.2608695652173913, 0.10526315789473685, 0.0, 0.2857142857142857, 0.0, 0.08695652173913043, 0.14285714285714288, 0.0, 0.14285714285714285, 0.2857142857142857, 0.0, 0.11764705882352941, 0.10526315789473685, 0.20000000000000004, 0.13333333333333333, 0.28571428571428575, 0.4, 0.09999999999999999, 0.30769230769230765, 0.0, 0.18181818181818182, 0.11764705882352941, 0.0, 0.11764705882352941, 0.1818181818181818, 0.26086956521739135, 0.3076923076923077, 0.5384615384615384, 0.2962962962962963, 0.30769230769230765, 0.18181818181818182, 0.1, 0.14285714285714285, 0.4705882352941177, 0.0, 0.08695652173913045, 0.11764705882352941, 0.0, 0.7368421052631577, 0.21428571428571427, 0.43478260869565216, 0.0, 0.34782608695652173, 0.16, 0.5454545454545454, 0.1, 0.09090909090909093, 0.33333333333333337, 0.34782608695652173, 0.25, 0.125, 0.21052631578947367, 0.39999999999999997, 0.125, 0.5, 0.5217391304347826, 0.5, 0.631578947368421, 0.0, 0.0, 0.0, 0.21052631578947367, 0.37499999999999994, 0.4, 0.14285714285714285, 0.11764705882352941, 0.0, 0.3157894736842105, 0.22222222222222224, 0.0, 0.0, 0.0, 0.2857142857142857, 0.34782608695652173, 0.0, 0.27272727272727276, 0.22222222222222224, 0.2608695652173913, 0.2105263157894737, 0.5333333333333333, 0.22222222222222224, 0.0, 0.3157894736842105, 0.22222222222222224, 0.16666666666666669, 0.11764705882352941, 0.4, 0.0, 0.0, 0.26666666666666666, 0.19047619047619047, 0.08, 0.1, 0.3076923076923077, 0.28571428571428575, 0.2105263157894737, 0.0, 0.19047619047619044, 0.25, 0.3703703703703704, 0.0, 0.2857142857142857, 0.0, 0.125, 0.588235294117647, 0.11764705882352941, 0.4444444444444444, 0.3333333333333333, 0.0, 0.0, 0.1111111111111111, 0.1904761904761905, 0.0, 0.26666666666666666, 0.08695652173913043, 0.0, 0.16666666666666666, 0.0, 0.1, 0.22222222222222224, 0.23076923076923075, 0.2105263157894737, 0.0, 0.125, 0.1, 0.39999999999999997, 0.14285714285714285, 0.5, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.2, 0.2222222222222222, 0.4444444444444444, 0.2222222222222222, 0.26666666666666666, 0.16666666666666666, 0.34782608695652173, 0.0, 0.2857142857142857, 0.0, 0.20000000000000004, 0.0, 0.1, 0.11764705882352942, 0.2105263157894737, 0.09999999999999999, 0.0, 0.3157894736842105, 0.2105263157894737, 0.14814814814814814, 0.0, 0.26666666666666666, 0.0, 0.0, 0.33333333333333326, 0.25, 0.3333333333333333, 0.37499999999999994, 0.0, 0.08695652173913043, 0.07407407407407407, 0.3636363636363636, 0.13333333333333333, 0.4444444444444445, 0.375, 0.13333333333333333, 0.13333333333333333, 0.16666666666666666, 0.6, 0.13333333333333333, 0.0, 0.3157894736842105, 0.2, 0.2608695652173913, 0.2857142857142857, 0.125, 0.0, 0.25, 0.42857142857142855, 0.08333333333333333, 0.13333333333333333, 0.08, 0.47058823529411764, 0.1111111111111111, 0.33333333333333337, 0.22222222222222224, 0.3636363636363636, 0.1, 0.11764705882352941, 0.25, 0.4444444444444444, 0.0, 0.35294117647058826, 0.1, 0.26666666666666666, 0.23529411764705882, 0.3157894736842105, 0.0, 0.29629629629629634, 0.0, 0.125, 0.125, 0.10526315789473685, 0.3529411764705882, 0.631578947368421, 0.34782608695652173, 0.25000000000000006, 0.0, 0.5263157894736842, 0.10526315789473685, 0.2105263157894737, 0.36363636363636365, 0.2222222222222222, 0.0, 0.11764705882352941, 0.2857142857142857, 0.0, 0.26666666666666666, 0.25, 0.10526315789473685, 0.17391304347826086, 0.0, 0.4615384615384615, 0.0, 0.14285714285714285, 0.18181818181818185, 0.2857142857142857, 0.32, 0.3157894736842105, 0.4210526315789474, 0.11764705882352941, 0.2608695652173913, 0.2222222222222222, 0.0, 0.13333333333333333, 0.3529411764705882, 0.21052631578947367, 0.13333333333333333, 0.2962962962962963, 0.23529411764705882, 0.11764705882352941, 0.07407407407407407, 0.08333333333333333, 0.0, 0.7777777777777777, 0.0, 0.4, 0.3, 0.2857142857142857, 0.0, 0.8333333333333333, 0.42857142857142855, 0.11764705882352941, 0.3157894736842105, 0.2222222222222222, 0.25, 0.0, 0.23076923076923075, 0.11764705882352941, 0.37499999999999994, 0.15384615384615385, 0.11111111111111112, 0.0, 0.20689655172413793, 0.34782608695652173, 0.5, 0.0, 0.26666666666666666, 0.2857142857142857, 0.125, 0.0, 0.0, 0.09523809523809522, 0.1, 0.14814814814814814, 0.0, 0.12500000000000003, 0.26666666666666666, 0.23529411764705882, 0.08333333333333334, 0.15384615384615383, 0.11764705882352941, 0.0, 0.33333333333333326, 0.0, 0.1111111111111111, 0.26666666666666666, 0.3157894736842105, 0.10526315789473685, 0.5454545454545454, 0.0, 0.1818181818181818, 0.0, 0.12500000000000003, 0.23529411764705882, 0.21052631578947367, 0.26666666666666666, 0.5714285714285714, 0.23076923076923075, 0.5454545454545454, 0.28571428571428575, 0.23529411764705882, 0.08333333333333333, 0.0, 0.22222222222222224, 0.11764705882352941, 0.11764705882352941, 0.0, 0.3076923076923077, 0.3157894736842105, 0.11764705882352941, 0.3636363636363636, 0.16666666666666666, 0.2727272727272727, 0.0, 0.35294117647058826, 0.16666666666666666, 0.18181818181818185, 0.14285714285714285, 0.47058823529411764, 0.18181818181818185, 0.36363636363636365, 0.0, 0.2, 0.625, 0.26086956521739135, 0.2, 0.11764705882352941, 0.2105263157894737, 0.1111111111111111, 0.3157894736842105, 0.4210526315789474, 0.09523809523809525, 0.6153846153846153, 0.09523809523809523, 0.25000000000000006, 0.5, 0.19047619047619044, 0.11764705882352941, 0.37499999999999994, 0.09523809523809525, 0.0, 0.0, 0.0, 0.5, 0.3529411764705882, 0.0, 0.0, 0.0, 0.5454545454545454, 0.33333333333333326, 0.33333333333333326, 0.14285714285714288, 0.0, 0.5555555555555556, 0.10526315789473685, 0.15384615384615383, 0.4, 0.13333333333333333, 0.5263157894736842, 0.08695652173913045, 0.15384615384615383, 0.13333333333333333, 0.08333333333333333, 0.0, 0.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.1, 0.1, 0.09090909090909091, 0.0, 0.0, 0.0, 0.11111111111111112, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11111111111111112, 0.0, 0.09090909090909091, 0.2105263157894737, 0.0, 0.0, 0.0, 0.0, 0.10526315789473682, 0.0, 0.0, 0.0, 0.10526315789473682, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.25, 0.1818181818181818, 0.0, 0.1, 0.11111111111111112, 0.2105263157894737, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.608695652173913, 0.2105263157894737, 0.23529411764705882, 0.0, 0.46153846153846156, 0.25, 0.10526315789473685, 0.1111111111111111, 0.1, 0.0, 0.09090909090909091, 0.3333333333333333, 0.0, 0.07142857142857144, 0.7692307692307693, 0.3157894736842105, 0.08695652173913043, 0.12500000000000003, 0.0, 0.36363636363636365, 0.2222222222222222, 0.3333333333333333, 0.28571428571428575, 0.10526315789473685, 0.15384615384615383, 0.2857142857142857, 0.4210526315789474, 0.19999999999999998, 0.6, 0.5555555555555556, 0.11764705882352941, 0.2, 0.11764705882352942, 0.2857142857142857, 0.4, 0.25, 0.4166666666666667, 0.0, 0.0, 0.2727272727272727, 0.10526315789473685, 0.25, 0.08333333333333333, 0.1111111111111111, 0.3636363636363636, 0.5263157894736842, 0.1, 0.30769230769230765, 0.3157894736842105, 0.26086956521739135, 0.11764705882352941, 0.21428571428571427, 0.1111111111111111, 0.3636363636363636, 0.15384615384615383, 0.28571428571428575, 0.0, 0.14285714285714288, 0.11764705882352941, 0.3, 0.22222222222222224, 0.1, 0.4444444444444444, 0.5555555555555556, 0.0, 0.22222222222222224, 0.25, 0.5714285714285714, 0.125, 0.2222222222222222, 0.13333333333333333, 0.13333333333333333, 0.0, 0.2105263157894737, 0.2222222222222222, 0.3157894736842105, 0.13333333333333333, 0.0, 0.125, 0.20000000000000004, 0.1111111111111111, 0.380952380952381, 0.25, 0.13333333333333333, 0.30769230769230765, 0.4, 0.3, 0.0, 0.1904761904761905, 0.0, 0.11764705882352942, 0.1818181818181818, 0.34782608695652173, 0.11764705882352941, 0.14285714285714285, 0.5, 0.36363636363636365, 0.21428571428571427, 0.4615384615384615, 0.1818181818181818, 0.5714285714285714, 0.11764705882352942, 0.25000000000000006, 0.2222222222222222, 0.1904761904761905, 0.16666666666666666, 0.08695652173913045, 0.2105263157894737, 0.11764705882352941, 0.375, 0.2608695652173913, 0.1111111111111111, 0.16, 0.30769230769230765, 0.0909090909090909, 0.3333333333333333, 0.35294117647058826, 0.09090909090909091, 0.26666666666666666, 0.0, 0.0, 0.2857142857142857, 0.30769230769230765, 0.18181818181818182, 0.11764705882352941, 0.0, 0.375, 0.0, 0.0, 0.4761904761904762, 0.11764705882352941, 0.1739130434782609, 0.21052631578947364, 0.42857142857142855, 0.28571428571428575, 0.39999999999999997, 0.14285714285714285, 0.14285714285714288, 0.375, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.09523809523809525, 0.19047619047619047, 0.0, 0.2105263157894737, 0.18181818181818182, 0.0, 0.30769230769230765, 0.11764705882352941, 0.11764705882352941, 0.26666666666666666, 0.09090909090909093, 0.1818181818181818, 0.5454545454545454, 0.2, 0.26666666666666666, 0.13333333333333333, 0.42857142857142855, 0.28571428571428564, 0.38095238095238093, 0.3529411764705882, 0.2105263157894737, 0.14285714285714288, 0.0, 0.1111111111111111, 0.16666666666666666, 0.16, 0.25, 0.42857142857142855, 0.36363636363636365, 0.2222222222222222, 0.0, 0.17391304347826086, 0.0, 0.2105263157894737, 0.5000000000000001, 0.1111111111111111, 0.125, 0.13333333333333333, 0.22222222222222224, 0.09090909090909091, 0.30769230769230765, 0.23529411764705882, 0.26666666666666666, 0.2857142857142857, 0.28571428571428575, 0.2105263157894737, 0.23529411764705882, 0.0, 0.26666666666666666, 0.14285714285714288, 0.3529411764705882, 0.09090909090909091, 0.0, 0.2, 0.33333333333333337, 0.0, 0.3157894736842105, 0.22222222222222224, 0.4444444444444444, 0.0, 0.37499999999999994, 0.5714285714285714, 0.5333333333333333, 0.21052631578947364, 0.11111111111111112, 0.3333333333333333, 0.1111111111111111, 0.26666666666666666, 0.11764705882352941, 0.30769230769230765, 0.09523809523809525, 0.125, 0.26666666666666666, 0.5454545454545454, 0.0, 0.28571428571428575, 0.23529411764705885, 0.3529411764705882, 0.0, 0.2727272727272727, 0.2222222222222222, 0.4444444444444444, 0.4210526315789474, 0.0, 0.0, 0.25, 0.07142857142857142, 0.47058823529411764, 0.1111111111111111, 0.0, 0.4, 0.13333333333333333, 0.1, 0.2857142857142857, 0.0, 0.1904761904761905, 0.125, 0.13333333333333333, 0.0, 0.4, 0.0, 0.0, 0.45454545454545453, 0.0, 0.1, 0.5185185185185186, 0.21052631578947367, 0.0, 0.2, 0.5, 0.0, 0.33333333333333337, 0.14285714285714285, 0.0, 0.0, 0.3333333333333333, 0.14285714285714285, 0.1111111111111111, 0.23529411764705885, 0.2857142857142857, 0.2, 0.3157894736842105, 0.0, 0.2105263157894737, 0.26666666666666666, 0.37499999999999994, 0.2, 0.25, 0.28571428571428564, 0.2857142857142857, 0.0, 0.5000000000000001, 0.3157894736842105, 0.42857142857142855, 0.7272727272727272, 0.22222222222222224, 0.0, 0.0, 0.25, 0.0, 0.28571428571428575, 0.2222222222222222, 0.0625, 0.2222222222222222, 0.11111111111111112, 0.2857142857142857, 0.19999999999999998, 0.5, 0.23529411764705885, 0.13333333333333333, 0.18181818181818182, 0.28571428571428575, 0.2857142857142857, 0.0, 0.14285714285714288, 0.23529411764705882, 0.125, 0.0, 0.30769230769230765, 0.42105263157894735, 0.23529411764705882, 0.11111111111111112, 0.1111111111111111, 0.0, 0.0, 0.2105263157894737, 0.0, 0.47058823529411764, 0.37499999999999994, 0.0, 0.0, 0.36363636363636365, 0.0, 0.15384615384615383, 0.16666666666666666, 0.15384615384615385, 0.5, 0.07142857142857142, 0.1111111111111111, 0.28571428571428564, 0.1818181818181818, 0.125, 0.11764705882352941, 0.13333333333333333, 0.125, 0.5555555555555556, 0.2857142857142857, 0.2727272727272727, 0.0, 0.5, 0.5217391304347827, 0.08, 0.2, 0.1111111111111111, 0.0, 0.37499999999999994, 0.0, 0.0, 0.3529411764705882, 0.125, 0.15384615384615385, 0.13333333333333333, 0.26666666666666666, 0.14285714285714285, 0.4615384615384615, 0.6086956521739131, 0.3529411764705882, 0.11764705882352941, 0.4615384615384615, 0.22222222222222224, 0.08695652173913045, 0.2, 0.0, 0.0, 0.0, 0.10526315789473685, 0.0, 0.26666666666666666, 0.3076923076923077, 0.0, 0.14285714285714288, 0.125, 0.1818181818181818, 0.13333333333333333, 0.5555555555555556, 0.10526315789473685, 0.19047619047619044, 0.23529411764705882, 0.0, 0.42857142857142855, 0.0, 0.11764705882352942, 0.375, 0.5, 0.0, 0.25000000000000006, 0.5, 0.0, 0.4, 0.37499999999999994, 0.11111111111111112, 0.1111111111111111, 0.0, 0.0, 0.12500000000000003, 0.14285714285714285, 0.3, 0.09090909090909091, 0.12500000000000003, 0.22222222222222224, 0.1111111111111111, 0.5263157894736842, 0.4705882352941177, 0.36363636363636365, 0.6250000000000001, 0.11764705882352941, 0.0, 0.3529411764705882, 0.3157894736842105, 0.37499999999999994, 0.11111111111111112, 0.11764705882352941, 0.1818181818181818, 0.26666666666666666, 0.0, 0.3333333333333333, 0.30769230769230765, 0.0, 0.0, 0.18181818181818182, 0.26666666666666666, 0.0, 0.0, 0.23529411764705882, 0.0, 0.09523809523809525, 0.2608695652173913, 0.15384615384615383, 0.0, 0.4, 0.25, 0.0, 0.125, 0.23529411764705882, 0.33333333333333326, 0.0, 0.1904761904761905, 0.34782608695652173, 0.1111111111111111, 0.3636363636363636, 0.4444444444444444, 0.3333333333333333, 0.0, 0.125, 0.10526315789473685, 0.0, 0.2857142857142857, 0.0, 0.6153846153846153, 0.09999999999999999, 0.23529411764705885, 0.3, 0.0, 0.15999999999999998, 0.25000000000000006, 0.19047619047619047, 0.16666666666666666, 0.5000000000000001, 0.0, 0.11764705882352941, 0.0, 0.0, 0.2105263157894737, 0.26666666666666666, 0.08695652173913043, 0.5714285714285714, 0.14285714285714288, 0.11764705882352941, 0.0, 0.13333333333333333, 0.33333333333333326, 0.12500000000000003, 0.20689655172413793, 0.3333333333333333, 0.25, 0.0, 0.14285714285714285, 0.33333333333333337, 0.6250000000000001, 0.13333333333333333, 0.10526315789473685, 0.15384615384615385, 0.07692307692307691, 0.0, 0.19047619047619047, 0.3333333333333333, 0.13333333333333333, 0.13333333333333333, 0.28571428571428575, 0.14285714285714288, 0.23529411764705882, 0.5, 0.1111111111111111, 0.5714285714285714, 0.15384615384615383, 0.3157894736842105, 0.2222222222222222, 0.26666666666666666, 0.0, 0.2, 0.14285714285714285, 0.23529411764705882, 0.2, 0.43478260869565216, 0.3076923076923077, 0.7692307692307692, 0.1111111111111111, 0.0, 0.1111111111111111, 0.0, 0.2222222222222222, 0.0, 0.0, 0.2727272727272727, 0.35294117647058826, 0.15384615384615385, 0.4210526315789474, 0.0, 0.4615384615384615, 0.3529411764705882, 0.13793103448275862, 0.0, 0.7272727272727272, 0.14285714285714285, 0.09090909090909091, 0.0, 0.0909090909090909, 0.0, 0.0, 0.0, 0.0, 0.10526315789473682, 0.26666666666666666, 0.09523809523809523, 0.0, 0.10526315789473682, 0.0, 0.0, 0.0, 0.0, 0.07692307692307691, 0.0, 0.09090909090909091, 0.09523809523809523, 0.0, 0.0, 0.0, 0.1, 0.0, 0.47058823529411764, 0.0, 0.0, 0.0, 0.30769230769230765, 0.11764705882352941, 0.08695652173913043, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.10526315789473682, 0.0, 0.1, 0.10526315789473682, 0.0, 0.23529411764705882, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.26666666666666666, 0.0, 0.1111111111111111, 0.11111111111111112, 0.15384615384615383, 0.6, 0.25, 0.16666666666666666, 0.16666666666666666, 0.3333333333333333, 0.2, 0.08333333333333333, 0.11764705882352941, 0.0909090909090909, 0.0, 0.0, 0.10526315789473685, 0.4615384615384615, 0.125, 0.1, 0.37499999999999994, 0.19047619047619047, 0.28571428571428575, 0.0, 0.13333333333333333, 0.3, 0.375, 0.23999999999999996, 0.4444444444444444, 0.26666666666666666, 0.25, 0.2105263157894737, 0.0, 0.13333333333333333, 0.3703703703703704, 0.0, 0.2222222222222222, 0.6666666666666667, 0.0, 0.3157894736842105, 0.0, 0.19999999999999998, 0.26666666666666666, 0.26666666666666666, 0.625, 0.2222222222222222, 0.0, 0.2727272727272727, 0.0, 0.35294117647058826, 0.6153846153846153, 0.15384615384615383, 0.11764705882352941, 0.380952380952381, 0.18181818181818182, 0.4210526315789474, 0.0, 0.0, 0.4, 0.30769230769230765, 0.28571428571428575, 0.13333333333333333, 0.0, 0.10526315789473685, 0.5333333333333333, 0.22222222222222224, 0.5, 0.1111111111111111, 0.5714285714285714, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.125, 0.0, 0.08695652173913043, 0.0, 0.08695652173913043, 0.37499999999999994, 0.18181818181818182, 0.0, 0.1111111111111111, 0.380952380952381, 0.2222222222222222, 0.0, 0.10526315789473684, 0.0, 0.2222222222222222, 0.6153846153846154, 0.39999999999999997, 0.1818181818181818, 0.33333333333333326, 0.0, 0.16666666666666666, 0.0, 0.3333333333333333, 0.0, 0.47058823529411764, 0.0, 0.26666666666666666, 0.26666666666666666, 0.3157894736842105, 0.375, 0.0, 0.4705882352941177, 0.3333333333333333, 0.0, 0.11764705882352941, 0.0, 0.0, 0.1818181818181818, 0.23999999999999996, 0.08333333333333333, 0.0, 0.16666666666666666, 0.0, 0.5217391304347825, 0.0, 0.14814814814814814, 0.0, 0.23529411764705882, 0.1818181818181818, 0.15384615384615383, 0.2, 0.23529411764705882, 0.35294117647058826, 0.2105263157894737, 0.25, 0.21052631578947367, 0.10526315789473685, 0.23529411764705882, 0.125, 0.2, 0.39999999999999997, 0.10526315789473685, 0.3157894736842105, 0.09999999999999999, 0.42105263157894735, 0.39999999999999997, 0.36363636363636365, 0.1, 0.2105263157894737, 0.11764705882352941, 0.5714285714285714, 0.0, 0.1818181818181818, 0.30769230769230765, 0.3, 0.13333333333333333, 0.0, 0.28571428571428575, 0.37499999999999994, 0.11764705882352941, 0.1111111111111111, 0.29629629629629634, 0.0, 0.26666666666666666, 0.3333333333333333, 0.07999999999999999, 0.1739130434782609, 0.1, 0.56, 0.5263157894736842, 0.0, 0.37499999999999994, 0.3636363636363636, 0.42857142857142855, 0.21052631578947367, 0.0, 0.125, 0.22222222222222224, 0.2222222222222222, 0.4210526315789474, 0.0, 0.1, 0.19047619047619044, 0.22222222222222224, 0.11111111111111112, 0.23529411764705882, 0.4444444444444444, 0.15384615384615383, 0.4615384615384615, 0.3333333333333333, 0.23529411764705882, 0.125, 0.0, 0.10526315789473685, 0.37499999999999994, 0.12500000000000003, 0.0, 0.48, 0.13333333333333333, 0.36363636363636365, 0.2, 0.10000000000000002, 0.09523809523809525, 0.0, 0.23529411764705885, 0.23529411764705882, 0.0, 0.5, 0.18181818181818182, 0.25, 0.1111111111111111, 0.25000000000000006, 0.11764705882352941, 0.4705882352941177, 0.1111111111111111, 0.125, 0.2857142857142857, 0.34782608695652173, 0.13333333333333333, 0.0, 0.631578947368421, 0.0, 0.2105263157894737, 0.4210526315789474, 0.4, 0.11764705882352941, 0.4210526315789474, 0.2857142857142857, 0.0, 0.11111111111111112, 0.4000000000000001, 0.24, 0.26666666666666666, 0.39999999999999997, 0.23529411764705882, 0.33333333333333326, 0.6666666666666666, 0.11764705882352941, 0.125, 0.3636363636363636, 0.2857142857142857, 0.0, 0.3333333333333333, 0.0, 0.0, 0.09090909090909091, 0.0, 0.09090909090909091, 0.37499999999999994, 0.33333333333333326, 0.24, 0.2105263157894737, 0.0, 0.39999999999999997, 0.3, 0.0, 0.0, 0.21052631578947367, 0.12500000000000003, 0.23529411764705882, 0.3076923076923077, 0.14285714285714285, 0.1111111111111111, 0.3529411764705882, 0.3333333333333333, 0.22222222222222224, 0.2105263157894737, 0.20000000000000004, 0.0, 0.380952380952381, 0.5263157894736842, 0.4615384615384615, 0.0, 0.2608695652173913, 0.5333333333333333, 0.0, 0.11111111111111112, 0.25, 0.25, 0.5, 0.25, 0.0, 0.2105263157894737, 0.0, 0.4, 0.15384615384615385, 0.10000000000000002, 0.14285714285714285, 0.12500000000000003, 0.0, 0.23529411764705882, 0.35294117647058826, 0.3, 0.17391304347826086, 0.75, 0.36363636363636365, 0.16666666666666666, 0.30769230769230765, 0.25000000000000006, 0.2105263157894737, 0.28571428571428575, 0.5263157894736842, 0.30769230769230765, 0.11111111111111112, 0.1904761904761905, 0.0, 0.32, 0.1, 0.23529411764705882, 0.25, 0.17391304347826086, 0.0, 0.0, 0.23529411764705885, 0.1111111111111111, 0.0, 0.0, 0.25, 0.5000000000000001, 0.2727272727272727, 0.08333333333333334, 0.11764705882352941, 0.0, 0.4444444444444444, 0.25000000000000006, 0.26666666666666666, 0.15384615384615385, 0.12500000000000003, 0.3636363636363636, 0.14285714285714288, 0.09090909090909093, 0.13333333333333333, 0.1111111111111111, 0.25, 0.26666666666666666, 0.35294117647058826, 0.16666666666666666, 0.10526315789473685, 0.0, 0.5263157894736842, 0.30769230769230765, 0.16666666666666666, 0.2222222222222222, 0.15384615384615383, 0.2222222222222222, 0.09090909090909091, 0.0, 0.5, 0.08, 0.13333333333333333, 0.13333333333333333, 0.125, 0.25, 0.0, 0.625, 0.631578947368421, 0.09090909090909091, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.2222222222222222, 0.37499999999999994, 0.19047619047619044, 0.25, 0.5, 0.23529411764705882, 0.125, 0.1111111111111111, 0.2857142857142857, 0.0, 0.23529411764705882, 0.35294117647058826, 0.2222222222222222, 0.26666666666666666, 0.1, 0.24000000000000005, 0.15384615384615385, 0.3529411764705882, 0.25, 0.2222222222222222, 0.2222222222222222, 0.11111111111111112, 0.21052631578947364, 0.08000000000000002, 0.20000000000000004, 0.3529411764705882, 0.42857142857142855, 0.26666666666666666, 0.0, 0.23529411764705882, 0.08333333333333334, 0.1, 0.5000000000000001, 0.15384615384615383, 0.33333333333333326, 0.125, 0.0, 0.5263157894736842, 0.0, 0.2857142857142857, 0.34782608695652173, 0.16666666666666666, 0.12500000000000003, 0.39999999999999997, 0.11764705882352941, 0.2727272727272727, 0.23076923076923075, 0.2857142857142857, 0.47058823529411764, 0.1111111111111111, 0.125, 0.23529411764705882, 0.42857142857142855, 0.2105263157894737, 0.0, 0.36363636363636365, 0.45454545454545453, 0.11764705882352941, 0.0, 0.28571428571428575, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.23529411764705885, 0.08333333333333333, 0.0, 0.47058823529411764, 0.0, 0.23529411764705882, 0.125, 0.28571428571428575, 0.5, 0.18181818181818182, 0.3157894736842105, 0.11764705882352941, 0.0, 0.3636363636363636, 0.2222222222222222, 0.38095238095238093, 0.0, 0.1111111111111111, 0.23529411764705882, 0.125, 0.4, 0.14285714285714288, 0.21052631578947367, 0.11764705882352941, 0.0, 0.0, 0.2, 0.43478260869565216, 0.3, 0.42857142857142855, 0.4615384615384615, 0.13333333333333333, 0.75, 0.6, 0.0, 0.0, 0.25, 0.375, 0.0, 0.25, 0.1, 0.25000000000000006, 0.13333333333333333, 0.10526315789473685, 0.28571428571428575, 0.4000000000000001, 0.11764705882352941, 0.22222222222222224, 0.380952380952381, 0.11764705882352941, 0.23529411764705882, 0.11764705882352941, 0.35294117647058826, 0.35294117647058826, 0.3076923076923077, 0.16666666666666666, 0.28571428571428575, 0.16666666666666666, 0.2, 0.0, 0.35294117647058826, 0.09090909090909093, 0.2727272727272727, 0.08695652173913043, 0.39999999999999997, 0.3157894736842105, 0.0, 0.5454545454545454, 0.08695652173913045, 0.16, 0.125, 0.26086956521739124, 0.1111111111111111, 0.35294117647058826, 0.15384615384615385, 0.3333333333333333, 0.125, 0.1739130434782609, 0.39999999999999997, 0.3333333333333333, 0.33333333333333337, 0.3157894736842105, 0.3, 0.2222222222222222, 0.3157894736842105, 0.18181818181818182, 0.35714285714285715, 0.22222222222222224, 0.4761904761904762, 0.3333333333333333, 0.2857142857142857, 0.3846153846153846, 0.125, 0.2105263157894737, 0.0, 0.08695652173913043, 0.2857142857142857, 0.125, 0.11764705882352942, 0.2857142857142857, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.33333333333333326, 0.0, 0.3333333333333333, 0.3157894736842105, 0.0, 0.11764705882352941, 0.34782608695652173, 0.11111111111111112, 0.0, 0.1, 0.1, 0.10526315789473682, 0.0, 0.1, 0.09523809523809523, 0.10526315789473682, 0.10526315789473682, 0.10526315789473682, 0.11111111111111112, 0.1, 0.09523809523809523, 0.22222222222222224, 0.09523809523809523, 0.13333333333333333, 0.08333333333333333, 0.0, 0.08695652173913043, 0.25, 0.08, 0.11111111111111112, 0.21052631578947367, 0.06451612903225808, 0.09523809523809523, 0.10526315789473682, 0.10526315789473684, 0.11111111111111112, 0.09090909090909091, 0.1, 0.10526315789473682, 0.1, 0.08695652173913043, 0.10526315789473682, 0.0, 0.09523809523809523, 0.10526315789473682, 0.125, 0.5263157894736842, 0.36363636363636365, 0.47058823529411764, 0.2222222222222222, 0.5, 0.6, 0.20000000000000004, 0.18181818181818182, 0.38095238095238093, 0.4, 0.125, 0.0, 0.23529411764705882, 0.0, 0.11764705882352941, 0.42105263157894735, 0.19999999999999998, 0.125, 0.3636363636363636, 0.5, 0.08695652173913043, 0.11764705882352941, 0.23529411764705882, 0.2962962962962963, 0.4615384615384615, 0.28571428571428575, 0.14285714285714288, 0.1904761904761905, 0.2, 0.1111111111111111, 0.08333333333333334, 0.26666666666666666, 0.2857142857142857, 0.0, 0.25, 0.09523809523809525, 0.37499999999999994, 0.3076923076923077, 0.25, 0.26666666666666666, 0.15384615384615383, 0.0, 0.26666666666666666, 0.19047619047619047, 0.5333333333333333, 0.1111111111111111, 0.5714285714285714, 0.3, 0.5217391304347826, 0.23529411764705882, 0.25, 0.3333333333333333, 0.125, 0.0, 0.4285714285714285, 0.13333333333333333, 0.11764705882352941, 0.0, 0.3157894736842105, 0.17391304347826086, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.1739130434782609, 0.2222222222222222, 0.3076923076923077, 0.3529411764705882, 0.0, 0.0909090909090909, 0.5714285714285714, 0.3, 0.10000000000000002, 0.12500000000000003, 0.11764705882352941, 0.13333333333333333, 0.3333333333333333, 0.39999999999999997, 0.26666666666666666, 0.13333333333333333, 0.23529411764705882, 0.18181818181818182, 0.0, 0.07692307692307691, 0.47058823529411764, 0.5454545454545455, 0.11764705882352941, 0.2857142857142857, 0.24000000000000005, 0.3157894736842105, 0.5714285714285714, 0.08695652173913045, 0.19047619047619047, 0.125, 0.5882352941176471, 0.625, 0.6, 0.375, 0.0, 0.11764705882352941, 0.2105263157894737, 0.30769230769230765, 0.5263157894736842, 0.26666666666666666, 0.19047619047619047, 0.20000000000000004, 0.10526315789473685, 0.14285714285714285, 0.25, 0.125, 0.12500000000000003, 0.14285714285714285, 0.26666666666666666, 0.4444444444444444, 0.7000000000000001, 0.23529411764705882, 0.29629629629629634, 0.1739130434782609, 0.25, 0.12500000000000003, 0.3333333333333333, 0.0, 0.0, 0.33333333333333326, 0.14285714285714288, 0.0, 0.10526315789473684, 0.10526315789473685, 0.5, 0.1111111111111111, 0.13333333333333333, 0.3, 0.125, 0.2857142857142857, 0.5333333333333333, 0.3, 0.12500000000000003, 0.23529411764705882, 0.16, 0.09523809523809525, 0.4444444444444445, 0.4, 0.09523809523809525, 0.5, 0.3157894736842105, 0.39999999999999997, 0.42857142857142855, 0.4444444444444444, 0.2, 0.0, 0.16666666666666666, 0.4210526315789474, 0.0, 0.39999999999999997, 0.33333333333333337, 0.25000000000000006, 0.0, 0.13333333333333333, 0.1111111111111111, 0.23529411764705882, 0.0, 0.09090909090909091, 0.35294117647058826, 0.08333333333333334, 0.37499999999999994, 0.22222222222222224, 0.5555555555555556, 0.3529411764705882, 0.4615384615384615, 0.25, 0.35294117647058826, 0.3157894736842105, 0.11764705882352941, 0.30769230769230765, 0.16666666666666666, 0.3333333333333333, 0.125, 0.3157894736842105, 0.4, 0.12500000000000003, 0.4999999999999999, 0.5185185185185186, 0.0909090909090909, 0.34782608695652173, 0.0, 0.0, 0.10526315789473685, 0.25000000000000006, 0.2857142857142857, 0.125, 0.1818181818181818, 0.5454545454545454, 0.41666666666666663, 0.16666666666666666, 0.09523809523809525, 0.23529411764705885, 0.4, 0.26666666666666666, 0.0, 0.5517241379310344, 0.26666666666666666, 0.23529411764705882, 0.35294117647058826, 0.0, 0.1111111111111111, 0.21052631578947367, 0.6666666666666665, 0.33333333333333337, 0.25, 0.5714285714285714, 0.1, 0.1739130434782609, 0.0, 0.0, 0.45454545454545453, 0.10526315789473685, 0.1111111111111111, 0.26666666666666666, 0.39999999999999997, 0.2608695652173913, 0.11111111111111112, 0.4615384615384615, 0.4444444444444445, 0.4, 0.11764705882352941, 0.0, 0.24999999999999994, 0.23529411764705882, 0.28571428571428575, 0.1818181818181818, 0.1904761904761905, 0.4, 0.0, 0.5454545454545454, 0.6666666666666666, 0.2105263157894737, 0.20000000000000004, 0.42857142857142855, 0.3636363636363636, 0.1111111111111111, 0.33333333333333326, 0.23529411764705882, 0.19047619047619047, 0.4, 0.3529411764705882, 0.4210526315789474, 0.25, 0.25000000000000006, 0.22222222222222224, 0.19047619047619047, 0.09523809523809523, 0.13333333333333333, 0.08695652173913045, 0.1111111111111111, 0.4210526315789474, 0.10526315789473684, 0.625, 0.11764705882352941, 0.19999999999999998, 0.5, 0.16666666666666666, 0.2105263157894737, 0.09523809523809523, 0.0, 0.21052631578947364, 0.13333333333333333, 0.2105263157894737, 0.5714285714285715, 0.375, 0.2727272727272727, 0.23529411764705882, 0.3157894736842105, 0.3, 0.0, 0.37499999999999994, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65df0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "d = [target, new, all]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('result_3.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
    "      wr = csv.writer(myfile)\n",
    "      wr.writerow((\"String\", \"new_string\",\"loss\"))\n",
    "      wr.writerows(export_data)\n",
    "myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2098452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3a225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd65712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8145d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20a2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(\n",
    "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = YourDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    # evaluating test dataset\n",
    "    console.log(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1a3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0c073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:41:48] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-base<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-7-574132b7a0b1&gt;:16</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:41:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-base\u001b[33m...\u001b[0m                                            \u001b[2m<ipython-input-7-574132b7a0b1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m16\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                       \u001b[2m                                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdbe4aff64049768b29ac5d8dbaa699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28165\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\28165\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3555b55a042ae8c71ceacdf5583dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28165\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd60fa63b814d0b8f96884948f23c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:43:30] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-7-574132b7a0b1&gt;:27</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:43:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                                \u001b[2m<ipython-input-7-574132b7a0b1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m27\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                       \u001b[2m                                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|   summarize: Saurav Kant, an alumnus of upGrad and     |  upGrad learner switches to career in ML &amp; Al with 90% |\n",
       "|IIIT-B's PG Program in Machine learning and Artificial  |                       salary hike                      |\n",
       "|Intelligence, was a Sr Systems Engineer at Infosys with |                                                        |\n",
       "|  almost 5 years of work experience. The program and    |                                                        |\n",
       "|     upGrad's 360-degree career support helped him      |                                                        |\n",
       "| transition to a Data Scientist at Tech Mahindra with   |                                                        |\n",
       "|  90% salary hike. upGrad's Online Power Learning has   |                                                        |\n",
       "|               powered 3 lakh+ careers.                 |                                                        |\n",
       "|   summarize: Kunal Shah's credit card bill payment     | Delhi techie wins free food from Swiggy for one year on|\n",
       "| platform, CRED, gave users a chance to win free food   |                          CRED                          |\n",
       "|   from Swiggy for one year. Pranav Kaushik, a Delhi    |                                                        |\n",
       "|  techie, bagged this reward after spending 2000 CRED   |                                                        |\n",
       "|coins. Users get one CRED coin per rupee of bill paid,  |                                                        |\n",
       "|  which can be used to avail rewards from brands like   |                                                        |\n",
       "|    Ixigo, BookMyShow, UberEats, Cult.Fit and more.     |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                      source_text                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                      target_text                      \u001b[0m|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|   summarize: Saurav Kant, an alumnus of upGrad and     |  upGrad learner switches to career in ML & Al with 90% |\n",
       "|IIIT-B's PG Program in Machine learning and Artificial  |                       salary hike                      |\n",
       "|Intelligence, was a Sr Systems Engineer at Infosys with |                                                        |\n",
       "|  almost 5 years of work experience. The program and    |                                                        |\n",
       "|     upGrad's 360-degree career support helped him      |                                                        |\n",
       "| transition to a Data Scientist at Tech Mahindra with   |                                                        |\n",
       "|  90% salary hike. upGrad's Online Power Learning has   |                                                        |\n",
       "|               powered 3 lakh+ careers.                 |                                                        |\n",
       "|   summarize: Kunal Shah's credit card bill payment     | Delhi techie wins free food from Swiggy for one year on|\n",
       "| platform, CRED, gave users a chance to win free food   |                          CRED                          |\n",
       "|   from Swiggy for one year. Pranav Kaushik, a Delhi    |                                                        |\n",
       "|  techie, bagged this reward after spending 2000 CRED   |                                                        |\n",
       "|coins. Users get one CRED coin per rupee of bill paid,  |                                                        |\n",
       "|  which can be used to avail rewards from brands like   |                                                        |\n",
       "|    Ixigo, BookMyShow, UberEats, Cult.Fit and more.     |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98401</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m98401\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78721</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m78721\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19680</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m19680\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-7-574132b7a0b1&gt;:85</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                            \u001b[2m<ipython-input-7-574132b7a0b1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m85\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                       \u001b[2m                                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(7.3582, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(7.3582, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"text\"] = \"summarize: \" + df[\"text\"]\n",
    "\n",
    "T5Trainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"text\",\n",
    "    target_text=\"headlines\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da4dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4550c4145b004b564f2b936b9955585751a2c488f90c30c541e98f3539ed2149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
