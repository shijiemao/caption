{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ea5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, AdamWeightDecay, \\\n",
    "    TFT5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f339f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\" Use tokenizer to preprocess data. \"\"\"\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    prefix = \"summarize: \"\n",
    "\n",
    "    inputs = [prefix + doc for doc in examples[\"string\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"label\"], max_length=80, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def download_and_preprocess_data(dataset):\n",
    "    \"\"\" Load dataset from HuggingFace and preprocess. \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    # Tokenized using preprocess_function\n",
    "    tokenized_news = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    return tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4c3146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "optimizer = AdamWeightDecay(\n",
    "    learning_rate=2e-5, \n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    return_tensors=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6fb4ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"test.xlsx\")\n",
    "dataset = Dataset.from_pandas(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0aed1c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b3d915f1b40d0889c50e9dc2bb405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28165\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['string', 'label', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 122\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_news = download_and_preprocess_data(dataset)\n",
    "tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1b8b94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tokenized_news[\"attention_mask\"]\n",
    "for i in range(len(attention_mask)):\n",
    "    for j in range(len(attention_mask[i])):\n",
    "        attention_mask[i][j] = np.int32(attention_mask[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d2187d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_news[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03009341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most apples are a little sweet and a little ta...</td>\n",
       "      <td>A description and explanation of how apples taste</td>\n",
       "      <td>1</td>\n",
       "      <td>[21603, 10, 1377, 16981, 33, 3, 9, 385, 2093, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[71, 4210, 11, 7295, 13, 149, 16981, 2373, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apples rank among the world's most popular fru...</td>\n",
       "      <td>a summary of different uses for apples</td>\n",
       "      <td>2</td>\n",
       "      <td>[21603, 10, 2184, 7, 11003, 859, 8, 296, 31, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 9, 9251, 13, 315, 2284, 21, 16981, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As white light passes through our atmosphere, ...</td>\n",
       "      <td>explanation of why the sky is blue</td>\n",
       "      <td>3</td>\n",
       "      <td>[21603, 10, 282, 872, 659, 9016, 190, 69, 4643...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[7295, 13, 572, 8, 5796, 19, 1692, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In other words, the color of the ocean and the...</td>\n",
       "      <td>a distinction between why the ocean is blue an...</td>\n",
       "      <td>4</td>\n",
       "      <td>[21603, 10, 86, 119, 1234, 6, 8, 945, 13, 8, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 9, 13005, 344, 572, 8, 5431, 19, 1692, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Color of Sunlight as seen on Earth's surface d...</td>\n",
       "      <td>explanation of why the sun is yellow</td>\n",
       "      <td>5</td>\n",
       "      <td>[21603, 10, 6088, 13, 3068, 2242, 38, 894, 30,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[7295, 13, 572, 8, 1997, 19, 4459, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Washington, D.C., D.C. in full District of Col...</td>\n",
       "      <td>specification of location of washignton DC</td>\n",
       "      <td>118</td>\n",
       "      <td>[21603, 10, 2386, 6, 309, 5, 254, 5, 6, 309, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[16726, 13, 1128, 13, 6179, 3191, 17, 106, 579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Concisely, AI can be described as the effort t...</td>\n",
       "      <td>comparison of AI and machine learning</td>\n",
       "      <td>119</td>\n",
       "      <td>[21603, 10, 1193, 75, 159, 15, 120, 6, 7833, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4993, 13, 7833, 11, 1437, 1036, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>How  many  layers  contribute  to  a model of ...</td>\n",
       "      <td>definition of the depth of a model</td>\n",
       "      <td>120</td>\n",
       "      <td>[21603, 10, 571, 186, 7500, 4139, 12, 3, 9, 82...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4903, 13, 8, 4963, 13, 3, 9, 825, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Functional anatomy Muscles Mucosa Lymphatics I...</td>\n",
       "      <td>list of functional anatomy</td>\n",
       "      <td>121</td>\n",
       "      <td>[21603, 10, 27155, 27782, 6887, 2482, 7, 4159,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[570, 13, 5014, 27782, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>The Fukushima accident was an accident in 201...</td>\n",
       "      <td>definition of the fukishima disaster</td>\n",
       "      <td>122</td>\n",
       "      <td>[21603, 10, 37, 6343, 2729, 7, 10813, 9, 3125,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4903, 13, 8, 7683, 2168, 7, 10813, 9, 6912, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                string  \\\n",
       "0    Most apples are a little sweet and a little ta...   \n",
       "1    Apples rank among the world's most popular fru...   \n",
       "2    As white light passes through our atmosphere, ...   \n",
       "3    In other words, the color of the ocean and the...   \n",
       "4    Color of Sunlight as seen on Earth's surface d...   \n",
       "..                                                 ...   \n",
       "117  Washington, D.C., D.C. in full District of Col...   \n",
       "118  Concisely, AI can be described as the effort t...   \n",
       "119  How  many  layers  contribute  to  a model of ...   \n",
       "120  Functional anatomy Muscles Mucosa Lymphatics I...   \n",
       "121   The Fukushima accident was an accident in 201...   \n",
       "\n",
       "                                                 label   id  \\\n",
       "0    A description and explanation of how apples taste    1   \n",
       "1               a summary of different uses for apples    2   \n",
       "2                   explanation of why the sky is blue    3   \n",
       "3    a distinction between why the ocean is blue an...    4   \n",
       "4                 explanation of why the sun is yellow    5   \n",
       "..                                                 ...  ...   \n",
       "117         specification of location of washignton DC  118   \n",
       "118              comparison of AI and machine learning  119   \n",
       "119                 definition of the depth of a model  120   \n",
       "120                         list of functional anatomy  121   \n",
       "121               definition of the fukishima disaster  122   \n",
       "\n",
       "                                             input_ids  \\\n",
       "0    [21603, 10, 1377, 16981, 33, 3, 9, 385, 2093, ...   \n",
       "1    [21603, 10, 2184, 7, 11003, 859, 8, 296, 31, 7...   \n",
       "2    [21603, 10, 282, 872, 659, 9016, 190, 69, 4643...   \n",
       "3    [21603, 10, 86, 119, 1234, 6, 8, 945, 13, 8, 5...   \n",
       "4    [21603, 10, 6088, 13, 3068, 2242, 38, 894, 30,...   \n",
       "..                                                 ...   \n",
       "117  [21603, 10, 2386, 6, 309, 5, 254, 5, 6, 309, 5...   \n",
       "118  [21603, 10, 1193, 75, 159, 15, 120, 6, 7833, 5...   \n",
       "119  [21603, 10, 571, 186, 7500, 4139, 12, 3, 9, 82...   \n",
       "120  [21603, 10, 27155, 27782, 6887, 2482, 7, 4159,...   \n",
       "121  [21603, 10, 37, 6343, 2729, 7, 10813, 9, 3125,...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "117  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "118  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "119  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "120  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \n",
       "0        [71, 4210, 11, 7295, 13, 149, 16981, 2373, 1]  \n",
       "1            [3, 9, 9251, 13, 315, 2284, 21, 16981, 1]  \n",
       "2                [7295, 13, 572, 8, 5796, 19, 1692, 1]  \n",
       "3    [3, 9, 13005, 344, 572, 8, 5431, 19, 1692, 11,...  \n",
       "4                [7295, 13, 572, 8, 1997, 19, 4459, 1]  \n",
       "..                                                 ...  \n",
       "117  [16726, 13, 1128, 13, 6179, 3191, 17, 106, 579...  \n",
       "118                [4993, 13, 7833, 11, 1437, 1036, 1]  \n",
       "119              [4903, 13, 8, 4963, 13, 3, 9, 825, 1]  \n",
       "120                          [570, 13, 5014, 27782, 1]  \n",
       "121    [4903, 13, 8, 7683, 2168, 7, 10813, 9, 6912, 1]  \n",
       "\n",
       "[122 rows x 6 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tokenized_news.to_pandas()\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b483617d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    716\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m                     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-b87eed8d62b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m test_ds = tokenized_news.to_tf_dataset(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m122\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mto_tf_dataset\u001b[1;34m(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m# TODO(Matt, QL): deprecate the retention of label_ids and label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         output_signature, columns_to_np_types = dataset._get_output_signature(\n\u001b[0m\u001b[0;32m    406\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mtest_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_to_retain\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mtest_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mtest_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcollate_fn_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[0mtest_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m    586\u001b[0m                     \u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         features = self.tokenizer.pad(\n\u001b[0m\u001b[0;32m    589\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3015\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3017\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m     def create_token_type_ids_from_sequences(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    731\u001b[0m                         \u001b[1;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                     )\n\u001b[1;32m--> 733\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    734\u001b[0m                     \u001b[1;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m                     \u001b[1;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "test_ds = tokenized_news.to_tf_dataset(\n",
    "    columns=[\"attention_mask\",\"input_ids\",\"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=122,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ac2eb746",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unrecognized array dtype object. \nNested types and image/audio types are not supported yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-da54550ae7b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m test_ds = tokenized_news.to_tf_dataset(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mto_tf_dataset\u001b[1;34m(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m# TODO(Matt, QL): deprecate the retention of label_ids and label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         output_signature, columns_to_np_types = dataset._get_output_signature(\n\u001b[0m\u001b[0;32m    406\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mtf_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m                 raise RuntimeError(\n\u001b[0m\u001b[0;32m    286\u001b[0m                     \u001b[1;34mf\"Unrecognized array dtype {np_arrays[0].dtype}. \\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[1;34m\"Nested types and image/audio types are not supported yet.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unrecognized array dtype object. \nNested types and image/audio types are not supported yet."
     ]
    }
   ],
   "source": [
    "test_ds = tokenized_news.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23c97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(metric, pred, actual):\n",
    "    \"\"\" Compute the model's rouge performance on an instance. \"\"\"\n",
    "\n",
    "    metric.add(predictions=pred, references=actual)\n",
    "    final_score = metric.compute()\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4c2650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-dd80fbf68f0d>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('rouge')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dd80fbf68f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "metric = load_metric('rouge')\n",
    "result = [[] for x in range(3)]\n",
    "\n",
    "cnt = 0\n",
    "for item in test_ds:\n",
    "    article = item['input_ids']\n",
    "    actual = item['labels']\n",
    "    \n",
    "    pred = model.generate(\n",
    "        do_sample=True,\n",
    "        input_ids=article,\n",
    "        # min_length=56,\n",
    "        max_length=80,\n",
    "        temperature=0.8, \n",
    "        top_k=45,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    rouge_score = compute_metrics(metric, pred, actual)\n",
    "    rouge1 = 100 * rouge_score['rouge1'][1][2]\n",
    "    rouge2 = 100 * rouge_score['rouge2'][1][2]\n",
    "    rougeL = 100 * rouge_score['rougeL'][1][2]\n",
    "\n",
    "    cnt += 1 \n",
    "    if cnt % 25 == 0:\n",
    "        print(f'Round: {cnt * 4}')\n",
    "\n",
    "    result[0].append(rouge1)\n",
    "    result[1].append(rouge2)\n",
    "    result[2].append(rougeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff0a3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.72463768115941,\n",
       " 25.517241379310345,\n",
       " 30.0,\n",
       " 17.721518987341774,\n",
       " 20.0,\n",
       " 23.154362416107382,\n",
       " 18.345323741007196,\n",
       " 27.666666666666668,\n",
       " 25.35714285714285,\n",
       " 20.921985815602838,\n",
       " 21.428571428571427,\n",
       " 23.36065573770492,\n",
       " 23.46938775510204,\n",
       " 22.887323943661972,\n",
       " 24.013157894736842,\n",
       " 22.535211267605636,\n",
       " 28.859060402684566,\n",
       " 30.000000000000004,\n",
       " 22.697368421052634,\n",
       " 28.24427480916031,\n",
       " 28.333333333333332,\n",
       " 20.833333333333336,\n",
       " 19.48529411764706,\n",
       " 19.327731092436974,\n",
       " 18.75,\n",
       " 19.485294117647058,\n",
       " 24.475524475524477,\n",
       " 20.542635658914726,\n",
       " 19.23076923076923,\n",
       " 30.00000000000001,\n",
       " 18.835616438356162,\n",
       " 19.178082191780824,\n",
       " 21.875,\n",
       " 21.428571428571427,\n",
       " 27.241379310344826,\n",
       " 21.48148148148148,\n",
       " 16.887417218543042,\n",
       " 22.10144927536232,\n",
       " 20.895522388059703,\n",
       " 18.750000000000004,\n",
       " 19.776119402985078,\n",
       " 19.666666666666668,\n",
       " 19.485294117647058,\n",
       " 27.04918032786885,\n",
       " 20.229007633587788,\n",
       " 20.370370370370374,\n",
       " 17.905405405405407,\n",
       " 22.468354430379744,\n",
       " 18.75,\n",
       " 17.64705882352941,\n",
       " 20.967741935483872,\n",
       " 17.375886524822697,\n",
       " 20.921985815602838,\n",
       " 25.17730496453901,\n",
       " 18.26923076923077,\n",
       " 18.987341772151904,\n",
       " 22.794117647058826,\n",
       " 32.720588235294116,\n",
       " 22.82608695652174,\n",
       " 25.37878787878788,\n",
       " 31.02189781021898,\n",
       " 20.353982300884955,\n",
       " 20.863309352517984,\n",
       " 22.18045112781955,\n",
       " 20.802919708029197,\n",
       " 26.623376623376622,\n",
       " 27.430555555555557,\n",
       " 22.569444444444446,\n",
       " 23.282442748091604,\n",
       " 26.296296296296294,\n",
       " 20.833333333333332,\n",
       " 18.749999999999996,\n",
       " 21.875,\n",
       " 21.95121951219512,\n",
       " 21.370967741935484,\n",
       " 16.28787878787879,\n",
       " 26.744186046511626,\n",
       " 31.118881118881113,\n",
       " 24.295774647887324,\n",
       " 23.728813559322035,\n",
       " 28.455284552845534,\n",
       " 18.620689655172416,\n",
       " 25.000000000000007,\n",
       " 21.38364779874214,\n",
       " 21.31782945736434,\n",
       " 27.142857142857142,\n",
       " 23.046874999999996,\n",
       " 15.384615384615385,\n",
       " 24.074074074074076,\n",
       " 21.962616822429904,\n",
       " 30.970149253731343,\n",
       " 22.01492537313433,\n",
       " 22.945205479452056,\n",
       " 21.95121951219512,\n",
       " 25.352112676056336,\n",
       " 20.000000000000004,\n",
       " 20.065789473684212,\n",
       " 23.015873015873016,\n",
       " 19.52054794520548,\n",
       " 20.645161290322584,\n",
       " 21.753246753246756,\n",
       " 16.556291390728475,\n",
       " 15.384615384615385,\n",
       " 18.620689655172416,\n",
       " 17.293233082706767,\n",
       " 30.681818181818183,\n",
       " 27.027027027027025,\n",
       " 23.000000000000004,\n",
       " 20.138888888888886,\n",
       " 23.376623376623375,\n",
       " 20.0,\n",
       " 20.945945945945947,\n",
       " 26.717557251908396,\n",
       " 22.368421052631582,\n",
       " 22.99270072992701,\n",
       " 23.809523809523807,\n",
       " 16.987179487179485,\n",
       " 16.77215189873418,\n",
       " 17.999999999999996,\n",
       " 22.131147540983605,\n",
       " 25.000000000000007,\n",
       " 17.628205128205128,\n",
       " 17.518248175182478,\n",
       " 32.79220779220779,\n",
       " 20.13888888888889,\n",
       " 28.947368421052634,\n",
       " 28.90625,\n",
       " 23.643410852713174,\n",
       " 19.594594594594593,\n",
       " 24.166666666666668,\n",
       " 30.09259259259259,\n",
       " 25.961538461538463,\n",
       " 18.439716312056742,\n",
       " 18.6046511627907,\n",
       " 25.833333333333336,\n",
       " 20.67669172932331,\n",
       " 20.666666666666668,\n",
       " 16.393442622950822,\n",
       " 15.584415584415584,\n",
       " 23.305084745762713,\n",
       " 23.404255319148938]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd93442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"The death toll from a strong earthquake in south-eastern Turkey, near Syria's border, could rise eight-fold, the World Health Organisation has warned.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a8e4ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = death toll from a strong earthquake in south-eastern Turkey could rise eightfold. the earthquake near Syria's border could rise 8fold, the world health organisation warns. a quake near the border could cause a tsunami.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_input = tokenizer(\"summarize: \" + test, max_length=1024, truncation=True, return_tensors='tf')\n",
    "\n",
    "pred = model.generate(\n",
    "    do_sample=True,\n",
    "    input_ids=tokenized_input['input_ids'],\n",
    "    min_length=56,\n",
    "    max_length=128,\n",
    "    temperature=0.8, \n",
    "    top_k=45,\n",
    "    no_repeat_ngram_size=3,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "pred_sentence = tokenizer.decode(pred[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"pred = {pred_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4550c4145b004b564f2b936b9955585751a2c488f90c30c541e98f3539ed2149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
