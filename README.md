

**Title:** 

**Who:**

- Advised by Prof Pradhan, Shekhar 
- Shijie Mao

**Introduction:**

**Related Works:**

**Text Summarization:**

**Text Generation &amp; QA:**

**Methodology:**

**A**

Our model architecture consists of two main parts: text summarization and Q&amp;A. In the first part (text summarization), we will be working with **T5** from Hugging Face Library. In the second part (Q&amp;A), we will be fine-tuning OpenAI&#39;s **GPT3** to adapt our dataset.


**Metrics:**

We plan to run two separate experiments for our summary-Q&amp;A model.

For the text summarization model, we will tune the hyperparameters by feeding a series of research articles into the model. This model&#39;s performance will be evaluated based on a variety of metrics - ROGUE &amp; BLEU (F1 score), and brevity penalty. This set of standards can be further researched and depends on the interests of the stakeholders.

For the second part, we will use existing Q&amp;A datasets such as _SQuAD2.0_. For performance evaluation, similar to the previous model, we will also be using ROGUE &amp; BLEU (F1 score). In addition, the EM (Exact Match) metric might be used to find the proportion of predictions that match the ground truth exactly. We would also experiment with the _top\_k_ parameter in the Q&amp;A pipeline.

Our base goal is to successfully implement our pipeline and make sure our summary-Q&amp;A model is generating reasonable results. Our target goal is to fine-tune the model to achieve good performance on both the summarization and Q&amp;A part, with F1 score (~0.4). Our stretch goal is to improve the entire pipeline to be able to achieve a relatively high F1 score (~0.6+) on mainstream testing datasets.

**Dataset:**

Manually labeled textbook paragraph 
The dataset generated by ask questions in google and manually labeled

**FutureWork:**

From the classified labeled dataset. Add a classification layer to the Q&amp;A  part to increase the model performance in different text environment. 
